{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'Customer-Churn-Records.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d249681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.read_csv(r'Customer-Churn-Records.csv')\n",
    "\n",
    "\n",
    "#print(df_encoded.columns)\n",
    "y = df['Exited'].astype(int)\n",
    "#df_encoded = df.astype(float)\n",
    "#df_encoded\n",
    "#X=df_encoded.drop(['Churn'], axis=1)\n",
    "\n",
    "df= df[['CreditScore', 'Age', 'Tenure', 'Balance',\n",
    "       'NumOfProducts', 'HasCrCard', 'EstimatedSalary',\n",
    "       'Exited', 'Satisfaction Score',\n",
    "       'Point Earned']]\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# 初始化LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "    \n",
    "X=df.drop(['Exited'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddd4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "for column in df.columns:\n",
    "    _, p_value  = stats.shapiro(df[column])\n",
    "    if p_value > 0.05:\n",
    "        print(f\"{column} seems to be normally distributed (p={p_value:.2f})\")\n",
    "    else:\n",
    "        print(f\"{column} does not seem to be normally distributed (p={p_value:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86125074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF baseline in the paper\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, matthews_corrcoef, confusion_matrix, classification_report)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Summarize class distribution\n",
    "print(\"Before SMOTE: \", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "\n",
    "# Apply SMOTE to generate synthetic samples\n",
    "smote = SMOTE(random_state=1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Summarize the new class distribution\n",
    "print(\"After SMOTE: \", dict(zip(*np.unique(y_resampled, return_counts=True))))\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the resampled dataset\n",
    "rf_classifier.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "\n",
    "y_prob = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# 評估模型\n",
    "print(\"AUC:\", auc_score)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e412eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, matthews_corrcoef, confusion_matrix, classification_report)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale_pos_weight\n",
    "scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)\n",
    "\n",
    "# parameter\n",
    "param_grid = {\n",
    "    'objective': ['binary:logistic'],\n",
    "    'scale_pos_weight': [scale_pos_weight],\n",
    "    'max_depth': [3,4,5],\n",
    "    'learning_rate': [0.1,0.01,0.2],\n",
    "    'n_estimators': [300,400,500],\n",
    "    'verbosity': [0]  # 使用verbosity取代過時的silent參數\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# \n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# \n",
    "print(\"AUC:\", auc_score)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Feature importance\n",
    "\n",
    "booster = model.get_booster()\n",
    "feature_importance = booster.get_score(importance_type='cover')\n",
    "\n",
    "# \n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "feature_names, feature_scores = zip(*sorted_features)\n",
    "\n",
    "# \n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_names, feature_scores)\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()  # 這行是為了將最重要的特徵放在頂部\n",
    "plt.show()\n",
    "feature_names,feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c406fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# \n",
    "booster = model.get_booster()\n",
    "feature_importance = booster.get_score(importance_type='weight')\n",
    "\n",
    "# \n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "feature_names, feature_scores = zip(*sorted_features)\n",
    "\n",
    "# \n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_names, feature_scores)\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis() \n",
    "plt.show()\n",
    "\n",
    "feature_names, feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "\n",
    "#Get shap value summary\n",
    "shap_values\n",
    "result = [np.mean([abs(items[i]) for items in shap_values]) for i in range(len(shap_values[0]))]\n",
    "\n",
    "#Get column name\n",
    "X.columns\n",
    "result\n",
    "# DataFrame\n",
    "df = pd.DataFrame({'Feature': X.columns, 'mean|shap value|': result})\n",
    "sorted_df = df.sort_values(by='mean|shap value|',ascending=False)\n",
    "# DataFrame\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc4af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "# ... (your code to train the model, etc.)\n",
    "\n",
    "# Assuming 'model' is your trained model\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Plotting the SHAP values with a beeswarm plot\n",
    "shap.summary_plot(shap_values, X_train,max_display=10,color=\"coolwarm\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.read_csv(r'Customer-Churn-Records.csv')\n",
    "\n",
    "\n",
    "#print(df_encoded.columns)\n",
    "y = df['Exited'].astype(int)\n",
    "#df_encoded = df.astype(float)\n",
    "#df_encoded\n",
    "#X=df_encoded.drop(['Churn'], axis=1)\n",
    "\n",
    "df= df[['CreditScore', 'Age', 'Tenure', 'Balance',\n",
    "       'NumOfProducts', 'IsActiveMember', 'EstimatedSalary',\n",
    "       'Exited', 'Satisfaction Score', \n",
    "       'Point Earned']]\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "    \n",
    "X=df.drop(['Exited'], axis=1)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\n",
    "X_temp=pd.concat([X_temp,y_temp],axis=1)\n",
    "X=X_temp[X_temp['Exited']==1]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9616fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes\n",
    "\n",
    "Xtofuzzy_Tri  = X[['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary','Satisfaction Score','Point Earned','IsActiveMember']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682900a9",
   "metadata": {},
   "source": [
    "## Fuzzy Triangle & Gaussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0b1099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Membership function\n",
    "def triangular_mf(x, a, b, c):\n",
    "    if x <= a:\n",
    "        return 0\n",
    "    elif a < x <= b:\n",
    "        return (x - a) / (b - a)\n",
    "    elif b < x <= c:\n",
    "        return (c - x) / (c - b)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Maximal cardinality\n",
    "def fuzzy_classification(x, min_val, median_val, max_val):\n",
    "    low_membership = triangular_mf(x, min_val, min_val, median_val)\n",
    "    medium_membership = triangular_mf(x, min_val, median_val, max_val)\n",
    "    high_membership = triangular_mf(x, median_val, max_val, max_val)\n",
    "    \n",
    "    max_membership = max(low_membership, medium_membership, high_membership)\n",
    "    \n",
    "    if max_membership == low_membership:\n",
    "        return 0\n",
    "    elif max_membership == medium_membership:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "  \n",
    "    \n",
    "fuzzy_df = pd.DataFrame()\n",
    "# For each feature\n",
    "for feature in Xtofuzzy_Tri.columns:\n",
    "    min_val = Xtofuzzy_Tri[feature].min()\n",
    "    median_val = Xtofuzzy_Tri[feature].quantile(0.5)\n",
    "    max_val = Xtofuzzy_Tri[feature].max()\n",
    "    \n",
    "    fuzzy_df[f'{feature}_fuzzy'] = Xtofuzzy_Tri[feature].apply(lambda x: fuzzy_classification(x, min_val, median_val, max_val))\n",
    "\n",
    "fuzzy_df\n",
    "\n",
    "# Define a dict\n",
    "boundaries = {}\n",
    "\n",
    "for feature in Xtofuzzy_Tri.columns:\n",
    "    min_val = Xtofuzzy_Tri[feature].min()\n",
    "    median_val = Xtofuzzy_Tri[feature].quantile(0.5)\n",
    "    max_val = Xtofuzzy_Tri[feature].max()\n",
    "    \n",
    "    # Save\n",
    "    boundaries[feature] = (min_val, median_val, max_val)\n",
    "\n",
    "# print out\n",
    "for feature, bounds in boundaries.items():\n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(f\"Low boundary: {bounds[0]}, Medium boundary: {bounds[1]}, High boundary: {bounds[2]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "fuzzy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c690267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Membership function\n",
    "def gaussian_mf(x, c, sigma):\n",
    "    return np.exp(-((x - c) ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "# 模糊分類函數\n",
    "def fuzzy_classification_g(x, c_low, c_mid, c_high, sigma):\n",
    "    low_membership = gaussian_mf(x, c_low, sigma)\n",
    "    medium_membership = gaussian_mf(x, c_mid, sigma)\n",
    "    high_membership = gaussian_mf(x, c_high, sigma)\n",
    "    \n",
    "    max_membership = max(low_membership, medium_membership, high_membership)\n",
    "    \n",
    "    if max_membership == low_membership:\n",
    "        return 0\n",
    "    elif max_membership == medium_membership:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# \n",
    "boundaries = {}\n",
    "output_df = pd.DataFrame()  \n",
    "\n",
    "# For each feature\n",
    "for feature in Xtofuzzy_Gaus.columns:\n",
    "    c_low = Xtofuzzy_Gaus[feature].quantile(0.25)\n",
    "    c_mid = Xtofuzzy_Gaus[feature].quantile(0.5)\n",
    "    c_high = Xtofuzzy_Gaus[feature].quantile(0.75)\n",
    "    sigma = Xtofuzzy_Gaus[feature].std()\n",
    "    \n",
    "    # 儲存分類邊界\n",
    "    boundaries[feature] = (c_low, c_mid, c_high)\n",
    "    \n",
    "    output_df[f'{feature}_fuzzy'] = Xtofuzzy_Gaus[feature].apply(lambda x: fuzzy_classification_g(x, c_low, c_mid, c_high, sigma))\n",
    "\n",
    "# \n",
    "feature_to_retrieve = \"TTL DAY MIN\"\n",
    "c_low, c_mid, c_high = boundaries[feature_to_retrieve]\n",
    "print(f\"c_low for TTL DAY MIN: {c_low}\")\n",
    "print(f\"c_mid for TTL DAY MIN: {c_mid}\")\n",
    "print(f\"c_high for TTL DAY MIN: {c_high}\")\n",
    "\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fuzzy_columns = [col for col in df_encoded.columns]\n",
    "\n",
    "# one-hot encoding\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=fuzzy_columns)\n",
    "\n",
    "# \n",
    "for col in fuzzy_columns:\n",
    "    mappings = {\n",
    "        f\"{col}_{i}\": f\"{col}_{i}\" for i in range(3)\n",
    "    }\n",
    "    df_encoded.rename(columns=mappings, inplace=True)\n",
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e491f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store dataset\n",
    "dataset = {}\n",
    "\n",
    "# Convert to transaction dataset\n",
    "transaction_id = 1\n",
    "for _, row in df_encoded.iterrows():\n",
    "    transaction = {}\n",
    "    for column, value in row.items():\n",
    "        transaction[column] = value\n",
    "    transaction_key = f'transaction{transaction_id}'\n",
    "    dataset[transaction_key] = transaction\n",
    "    transaction_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ddb494",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = dataset\n",
    "\n",
    "filtered_transactions = {transaction: {key: value for key, value in items.items() if value != 0}\n",
    "                        for transaction, items in transactions.items()}\n",
    "\n",
    "filtered_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece1fe93",
   "metadata": {},
   "source": [
    "## High Utility fuzzy Churn Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d52140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_base_name(fuzzy_name):\n",
    "    if fuzzy_name.startswith(\"Status_\"):\n",
    "        return \"Status\"\n",
    "    elif fuzzy_name.startswith(\"Age Group_\"):\n",
    "        return \"Age Group\"\n",
    "    elif fuzzy_name.startswith(\"Tariff Plan_\"):\n",
    "        return \"Tariff Plan\"\n",
    "    elif \"fuzzy\" in fuzzy_name:\n",
    "        # Splitting at the first underscore and taking the first part\n",
    "        return fuzzy_name.split(\"_\", 1)[0]\n",
    "    else:\n",
    "        return fuzzy_name\n",
    "\n",
    "def calculate_utility(itemset, transactions, profit_table):\n",
    "    \"\"\"\n",
    "    计算项目集的效用总和\n",
    "    \"\"\"\n",
    "    total_utility = 0\n",
    "    for transaction in transactions.values():\n",
    "        # 检查项目集是否包含在交易中\n",
    "        if set(itemset).issubset(set(transaction.keys())):\n",
    "            # 加总项目的价值\n",
    "            for item, utility in transaction.items():\n",
    "                if item in itemset:\n",
    "                    # 使用基本名称从profit_table查询利润\n",
    "                    base_name = extract_base_name(item)\n",
    "                    total_utility += utility * profit_table[base_name]\n",
    "    return total_utility\n",
    "\n",
    "def find_top_k_high_utility_itemsets(transactions, profit_table, k):\n",
    "    \"\"\"\n",
    "    找出前K個高效用項目集\n",
    "    \"\"\"\n",
    "    itemsets = set()\n",
    "    for transaction in transactions.values():\n",
    "        # 將交易中的每個項目加入項目集\n",
    "        itemsets.update(transaction.keys())\n",
    "\n",
    "    # 儲存高效用項目集及其效用總和\n",
    "    high_utility_itemsets = {}\n",
    "\n",
    "    for item in itemsets:\n",
    "        # 計算單一項目的效用總和\n",
    "        utility = calculate_utility([item], transactions, profit_table)\n",
    "        if utility > 0:\n",
    "            high_utility_itemsets[(item,)] = utility\n",
    "\n",
    "    # 生成更大的項目集\n",
    "    P_itemsets = {}\n",
    "    while True:\n",
    "        temp_itemsets = {}\n",
    "        for itemset, utility in high_utility_itemsets.items():\n",
    "            for item in itemsets:\n",
    "                if item not in itemset:\n",
    "                    new_itemset = tuple(sorted(list(itemset) + [item]))\n",
    "                    new_utility = calculate_utility(new_itemset, transactions, profit_table)\n",
    "                    if new_utility > 0:\n",
    "                        temp_itemsets[new_itemset] = new_utility\n",
    "                        P_itemsets[new_itemset] = new_utility\n",
    "\n",
    "        # 若 temp_itemsets 為空，停止迭代\n",
    "        if not temp_itemsets:\n",
    "            break\n",
    "\n",
    "        # 選取前K個高效用項目集\n",
    "        sorted_itemsets = sorted(temp_itemsets.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_k_itemsets = sorted_itemsets[:k]\n",
    "        high_utility_itemsets = {itemset: utility for itemset, utility in top_k_itemsets}\n",
    "\n",
    "    return high_utility_itemsets, P_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP\n",
    "profit_table = {\n",
    "   \"Age\": 1.051942,\n",
    "    \"NumOfProducts\": 0.944460,\n",
    "    \"Balance\": 0.375520,\n",
    "    \"EstimatedSalary\": 0.275761,\n",
    "    \"CreditScore\": 0.285694,\n",
    "    \"Point Earned\": 0.258406,\n",
    "    \"Tenure\": 0.162138,\n",
    "    \"Satisfaction Score\": 0.143929,\n",
    "    \"HasCrCard\": 0.045952,\n",
    "    \"IsActiveMember\":0.510511,\n",
    "    \"Card Type\":0.088436,\n",
    "    \"Geography\":0.277061,\n",
    "    \"Gender\":0.293275\n",
    "}\n",
    "\n",
    "\n",
    "# 取得前2個高效用項目集\n",
    "result,P_itemsets= find_top_k_high_utility_itemsets(filtered_transactions, profit_table, 3)\n",
    "\n",
    "#print(P_itemsets)\n",
    "sorted_data = sorted(P_itemsets.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Top 10\n",
    "top_10 = sorted_data[:10]\n",
    "top_10 = {itemset: value for itemset, value in top_10}\n",
    "#print(top_10)\n",
    "print(\"Top 10 High Utility Itemsets:\")\n",
    "for itemset, utility in top_10.items():\n",
    "    print(itemset, \"-> Utility:\", round(utility,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca80c8",
   "metadata": {},
   "source": [
    "## High Utility Length=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_high_utility_itemsets(transactions, profit_table, k, desired_length):\n",
    "    \"\"\"\n",
    "    找出前K個高效用項目集\n",
    "    \"\"\"\n",
    "    itemsets = set()\n",
    "    for transaction in transactions.values():\n",
    "        # 將交易中的每個項目加入項目集\n",
    "        itemsets.update(transaction.keys())\n",
    "\n",
    "    # 儲存高效用項目集及其效用總和\n",
    "    high_utility_itemsets = {}\n",
    "    P_itemsets = {}\n",
    "\n",
    "    for item in itemsets:\n",
    "        # 計算單一項目的效用總和\n",
    "        utility = calculate_utility([item], transactions, profit_table)\n",
    "        if utility > 0:\n",
    "            high_utility_itemsets[(item,)] = utility\n",
    "\n",
    "    while True:\n",
    "        temp_itemsets = {}\n",
    "        for itemset, utility in high_utility_itemsets.items():\n",
    "            for item in itemsets:\n",
    "                if item not in itemset:\n",
    "                    new_itemset = tuple(sorted(list(itemset) + [item]))\n",
    "                    new_utility = calculate_utility(new_itemset, transactions, profit_table)\n",
    "                    if new_utility > 0 and len(new_itemset) <= desired_length:\n",
    "                        temp_itemsets[new_itemset] = new_utility\n",
    "                        P_itemsets[new_itemset] = new_utility\n",
    "\n",
    "        # 若 temp_itemsets 為空，停止迭代\n",
    "        if not temp_itemsets:\n",
    "            break\n",
    "\n",
    "        # 選取前K個高效用項目集\n",
    "        sorted_itemsets = sorted(temp_itemsets.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_k_itemsets = sorted_itemsets[:k]\n",
    "        high_utility_itemsets = {itemset: utility for itemset, utility in top_k_itemsets}\n",
    "\n",
    "    return high_utility_itemsets, P_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87816843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP\n",
    "profit_table = {\n",
    "   \"Age\": 1.051942,\n",
    "    \"NumOfProducts\": 0.944460,\n",
    "    \"Balance\": 0.375520,\n",
    "    \"EstimatedSalary\": 0.275761,\n",
    "    \"CreditScore\": 0.285694,\n",
    "    \"Point Earned\": 0.258406,\n",
    "    \"Tenure\": 0.162138,\n",
    "    \"Satisfaction Score\": 0.143929,\n",
    "    \"HasCrCard\": 0.045952,\n",
    "    \"IsActiveMember\":0.510511,\n",
    "    \"Card Type\":0.088436,\n",
    "    \"Geography\":0.277061,\n",
    "    \"Gender\":0.293275\n",
    "}\n",
    "\n",
    "# 取得前2個高效用項目集\n",
    "result,P_itemsets= find_top_k_high_utility_itemsets(filtered_transactions, profit_table, 3,2)\n",
    "\n",
    "# 輸出結果\n",
    "#print(P_itemsets)\n",
    "# 使用sorted函式將字典按數值降序排序\n",
    "sorted_data = sorted(P_itemsets.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 取得前5名的資料\n",
    "top_10 = sorted_data[:10]\n",
    "top_10 = {itemset: value for itemset, value in top_10}\n",
    "#print(top_3)\n",
    "print(\"Top 10 High Utility Itemsets:\")\n",
    "for itemset, utility in top_10.items():\n",
    "    print(itemset, \"-> Utility:\", round(utility,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d5275",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.read_csv(r'Customer-Churn-Records.csv')\n",
    "\n",
    "\n",
    "\n",
    "y = df['Exited'].astype(int)\n",
    "\n",
    "\n",
    "df= df[['CreditScore', 'Age', 'Tenure', 'Balance',\n",
    "       'NumOfProducts', 'IsActiveMember', 'EstimatedSalary',\n",
    "       'Exited', 'Satisfaction Score', \n",
    "       'Point Earned']]\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# LabelEncoder\n",
    "label_encoder = LabelEncoder()   \n",
    "    \n",
    "X=df.drop(['Exited'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "fuzzy_df = pd.DataFrame()\n",
    "# Fuzzy classification\n",
    "for feature in Xtofuzzy_Tri.columns:\n",
    "    min_val = Xtofuzzy_Tri[feature].min()\n",
    "    median_val = Xtofuzzy_Tri[feature].quantile(0.5)\n",
    "    max_val = Xtofuzzy_Tri[feature].max()\n",
    "    \n",
    "    fuzzy_df[f'{feature}_fuzzy'] = Xtofuzzy_Tri[feature].apply(lambda x: fuzzy_classification(x, min_val, median_val, max_val))\n",
    "\n",
    "\n",
    "df_fuzzy = pd.concat([fuzzy_df],axis=1)\n",
    "\n",
    "fuzzy_columns = [col for col in df_fuzzy.columns]\n",
    "\n",
    "# one-hot encoding\n",
    "df_fuzzy = pd.get_dummies(df_fuzzy, columns=fuzzy_columns)\n",
    "df_fuzzy =df_fuzzy.astype(int)\n",
    "df_fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b07b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.read_csv(r'Customer-Churn-Records.csv')\n",
    "\n",
    "\n",
    "#print(df_encoded.columns)\n",
    "y = df['Exited'].astype(int)\n",
    "#df_encoded = df.astype(float)\n",
    "#df_encoded\n",
    "#X=df_encoded.drop(['Churn'], axis=1)\n",
    "\n",
    "df= df[['CreditScore', 'Age', 'Tenure', 'Balance',\n",
    "       'NumOfProducts', 'HasCrCard', 'EstimatedSalary',\n",
    "       'Exited', 'Satisfaction Score',\n",
    "       'Point Earned']]\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 patterns\n",
    "\n",
    "\n",
    "('Age_fuzzy_1', 'NumOfProducts_fuzzy_0') -> Utility: 2012.0\n",
    "('Age_fuzzy_1', 'CreditScore_fuzzy_1', 'NumOfProducts_fuzzy_0') -> Utility: 1725.0\n",
    "('Age_fuzzy_1', 'Balance_fuzzy_1', 'NumOfProducts_fuzzy_0') -> Utility: 1713.0\n",
    "('Age_fuzzy_1', 'IsActiveMember_fuzzy_0', 'NumOfProducts_fuzzy_0') -> Utility: 1650.0\n",
    "('Age_fuzzy_1', 'Balance_fuzzy_1') -> Utility: 1496.0\n",
    "('Age_fuzzy_1', 'CreditScore_fuzzy_1') -> Utility: 1487.0\n",
    "('Age_fuzzy_1', 'IsActiveMember_fuzzy_0') -> Utility: 1469.0\n",
    "('Age_fuzzy_1', 'Balance_fuzzy_1', 'CreditScore_fuzzy_1', 'NumOfProducts_fuzzy_0') -> Utility: 1438.0\n",
    "('Age_fuzzy_1', 'CreditScore_fuzzy_1', 'IsActiveMember_fuzzy_0', 'NumOfProducts_fuzzy_0') -> Utility: 1377.0\n",
    "('Age_fuzzy_1', 'Balance_fuzzy_1', 'IsActiveMember_fuzzy_0', 'NumOfProducts_fuzzy_0') -> Utility: 1366.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(df, df_fuzzy, left_index=True, right_index=True, how='left')\n",
    "X['fuzzyp1']=X['Age_fuzzy_1']*X['NumOfProducts_fuzzy_0']\n",
    "X['fuzzyp2']=X['Age_fuzzy_1']*X['CreditScore_fuzzy_1']*X['NumOfProducts_fuzzy_0']\n",
    "X['fuzzyp3']=X['Age_fuzzy_1']*X['Balance_fuzzy_1']*X['NumOfProducts_fuzzy_0']\n",
    "X['fuzzyp4']=X['Age_fuzzy_1']*X['NumOfProducts_fuzzy_0']*X['IsActiveMember_fuzzy_0']\n",
    "X['fuzzyp5']=X['Age_fuzzy_1']*X['Balance_fuzzy_1']\n",
    "X['fuzzyp6']=X['Age_fuzzy_1']*X['CreditScore_fuzzy_1']\n",
    "X['fuzzyp7']=X['Age_fuzzy_1']*X['IsActiveMember_fuzzy_0']\n",
    "X['fuzzyp8']=X['Age_fuzzy_1']*X['Balance_fuzzy_1']*X['CreditScore_fuzzy_1']*X['NumOfProducts_fuzzy_0']\n",
    "X['fuzzyp9']=X['Age_fuzzy_1']*X['CreditScore_fuzzy_1']*X['IsActiveMember_fuzzy_0']*X['NumOfProducts_fuzzy_0']\n",
    "X['fuzzyp10']=X['Age_fuzzy_1']*X['Balance_fuzzy_1']*X['IsActiveMember_fuzzy_0']*X['NumOfProducts_fuzzy_0']\n",
    "\n",
    "\n",
    "#*X['Subscription  Length_fuzzy_1']*X['Tariff Plan_1']\n",
    "#X=X[X['fuzzyp']==0]\n",
    "#y=X[X['CHURN','fuzzyp']]\n",
    "X=X.drop(['CreditScore_fuzzy_0', 'CreditScore_fuzzy_1', 'CreditScore_fuzzy_2',\n",
    "       'Age_fuzzy_0', 'Age_fuzzy_1', 'Age_fuzzy_2', 'Tenure_fuzzy_0',\n",
    "       'Tenure_fuzzy_1', 'Tenure_fuzzy_2', 'Balance_fuzzy_0',\n",
    "       'Balance_fuzzy_1', 'Balance_fuzzy_2', 'NumOfProducts_fuzzy_0',\n",
    "       'NumOfProducts_fuzzy_1', 'NumOfProducts_fuzzy_2',\n",
    "       'IsActiveMember_fuzzy_0', 'IsActiveMember_fuzzy_2',\n",
    "       'EstimatedSalary_fuzzy_0', 'EstimatedSalary_fuzzy_1',\n",
    "       'EstimatedSalary_fuzzy_2', 'Exited_fuzzy_0',\n",
    "       'Satisfaction Score_fuzzy_0', 'Satisfaction Score_fuzzy_1',\n",
    "       'Satisfaction Score_fuzzy_2', 'Point Earned_fuzzy_0',\n",
    "       'Point Earned_fuzzy_1', 'Point Earned_fuzzy_2','Exited'],axis=1)\n",
    "X = X.fillna(0)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8962aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 分割數據集為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# 計算scale_pos_weight\n",
    "scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)\n",
    "\n",
    "# 設定參數\n",
    "param_grid = {\n",
    "    'objective': ['binary:logistic'],\n",
    "    'scale_pos_weight': [scale_pos_weight],\n",
    "    'max_depth': [3,4,5],\n",
    "    'learning_rate': [0.1,0.01,0.2],\n",
    "    'n_estimators': [300,400,500],\n",
    "    'verbosity': [0]  # 使用verbosity取代過時的silent參數\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "#clf = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#print(\"Best Parameters:\", clf.best_params_)\n",
    "\n",
    "#best_model = clf.best_estimator_\n",
    "\n",
    "# 預測\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 評估模型\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# 評估模型\n",
    "print(\"AUC:\", auc_score)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, matthews_corrcoef, confusion_matrix, classification_report)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Summarize class distribution\n",
    "print(\"Before SMOTE: \", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "\n",
    "# Apply SMOTE to generate synthetic samples\n",
    "smote = SMOTE(random_state=1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Summarize the new class distribution\n",
    "print(\"After SMOTE: \", dict(zip(*np.unique(y_resampled, return_counts=True))))\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the resampled dataset\n",
    "rf_classifier.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "\n",
    "y_prob = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# 評估模型\n",
    "print(\"AUC:\", auc_score)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e45674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
